{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parameters: \n",
    "* one hidden layer with three neurons, to avoid overfitting; \n",
    "* L-FBGS as a solver, which performs well with small samples; \n",
    "* an alpha set to 0.05 for fast convergence, \n",
    "* a constant learning rate for fast training, \n",
    "* and logistic activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import seaborn as sns\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "def average(lst): \n",
    "    return sum(lst) / len(lst) \n",
    "days = [1,2,3,4,5,6,7,14,21,28]\n",
    "# IMPORTANT: Seeds to try\n",
    "seeds = [1,2,3,4,5]\n",
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "normalizer = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns Description\n",
    "\n",
    "*Corresponding to the number of instances before a certain polling release date (e.g 24 XPosts located on a 1_1_ file belong to the number of X posts for candidate 1 over a span of 1 day before a specified date)*\n",
    "\n",
    "* XPosts: Number of overall posts in X (Twitter)\n",
    "* Xcomments: Number of overall comments in X\n",
    "* XRts: Number of overall Rt´s in X\n",
    "* XLikes: Number of overall likes in X\n",
    "* XCommsPPost: Average number of comments per post for X\n",
    "* XRtsPPost: Average number of Rts per post for X\n",
    "* XLikesPPost: Average number of likes per post for X\n",
    "\n",
    "* FBPosts: Number of overall posts in Facebook\n",
    "* FBReactions: Number of overall reactions in Facebook\n",
    "* FBComments: Number of overall comments in Facebook\n",
    "* FBShares: Number of overall comments in Facebook\n",
    "* FBCommsPPost: Average number of comments per post for Facebook\n",
    "* FBReactsPPost: Average number of reactions per post for Facebook\n",
    "* FBLikesPPost: Average number of likes per post for Facebook\n",
    "\n",
    "* IGPosts: Number of overall posts in Instagram\n",
    "* IGLikes: Number of overall likes in Instagram\n",
    "* IGLikesPPost: Average number of likes per post for Instagram\n",
    "\n",
    "* YTPosts: Number of overall posts in YouTube\n",
    "* YTViews: Number of overall views in YouTube\n",
    "* YTViewsPPost: Average number of views per post for YouTube\n",
    "\n",
    "* Target: the reported vote share for the candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting columns to use (see New_DB)\n",
    "columns = ['XPosts', 'Xcomments', 'XRts', 'Xlikes', 'XCommsPPost', 'XRTsPPost', 'XlikesPPost', 'FBPosts', 'FBReactions', 'FBComments', 'FBShares', 'FBReactsPPost', 'FBCommsPPost', 'FBSharesPPost', 'IGPosts', 'IGLikes', 'IGLikesPPost', 'YTPosts', 'YTViews', 'YTViewsPPost', 'Target']\n",
    "\n",
    "target = ['Target']\n",
    "\n",
    "feature_columns_all = ['XPosts', 'Xcomments', 'XRts', 'Xlikes', 'XCommsPPost', 'XRTsPPost', 'XlikesPPost', 'FBPosts', 'FBReactions', 'FBComments', 'FBShares', 'FBReactsPPost', 'FBCommsPPost', 'FBSharesPPost', 'IGPosts', 'IGLikes', 'IGLikesPPost', 'YTPosts', 'YTViews', 'YTViewsPPost']\n",
    "\n",
    "feature_columns_notall = ['XPosts', 'Xcomments', 'XRts', 'Xlikes', 'XCommsPPost', 'XRTsPPost', 'XlikesPPost', 'FBPosts', 'FBReactions', 'FBComments', 'FBShares', 'FBReactsPPost', 'FBCommsPPost', 'FBSharesPPost']\n",
    "\n",
    "testing_columns = ['XPosts', 'Xcomments', 'XRts', 'Xlikes', 'XCommsPPost', 'XRTsPPost', 'XlikesPPost']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for plotting feature importance\n",
    "def plot_features(columns, importances, length):\n",
    "    df = (pd.DataFrame({\"features\": columns, \"feature_importance\": importances}) .sort_values(\"feature_importance\", ascending=False) .reset_index(drop=True))\n",
    "    sns.barplot(x=\"feature_importance\", y=\"features\", data=df[:length], orient=\"h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features including all features: 20\n",
      "Number of features including only Facebook and X: 14\n",
      "Number of features including some: 7\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of features including all features: {len(feature_columns_all)}\")\n",
    "print(f\"Number of features including only Facebook and X: {len(feature_columns_notall)}\")\n",
    "print(f\"Number of features including some: {len(testing_columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPRegressor(activation=&#x27;logistic&#x27;, alpha=0.05, hidden_layer_sizes=(3,),\n",
       "             max_iter=10000, solver=&#x27;lbfgs&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPRegressor</label><div class=\"sk-toggleable__content\"><pre>MLPRegressor(activation=&#x27;logistic&#x27;, alpha=0.05, hidden_layer_sizes=(3,),\n",
       "             max_iter=10000, solver=&#x27;lbfgs&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPRegressor(activation='logistic', alpha=0.05, hidden_layer_sizes=(3,),\n",
       "             max_iter=10000, solver='lbfgs')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr = MLPRegressor(solver=\"lbfgs\", hidden_layer_sizes=(3,), alpha=0.05, learning_rate=\"constant\", activation=\"logistic\", max_iter=10000)\n",
    "regr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xóchitl Gálvez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = regr\n",
    "predictions = []\n",
    "\n",
    "features_included = feature_columns_all\n",
    "\n",
    "for i in days:\n",
    "  # Scan the file and set data\n",
    "  data = pd.read_csv(f'../galvez/2_{i}.csv', usecols=columns, encoding=\"utf-8\")\n",
    "  # Training and testing data; Remove last row which is the testing row\n",
    "  training = data.iloc[:-1]\n",
    "  testing = pd.DataFrame(data.iloc[-1])\n",
    "  testing = testing.T\n",
    "\n",
    "  # Features and target columns as NumPy arrays\n",
    "  X_train = training[features_included].values\n",
    "  X_test = testing[features_included].values\n",
    "  y_train = training[target].values.ravel()  # Flatten y_train to 1D array\n",
    "  y_test = testing[target].values.ravel()\n",
    "\n",
    "  x_train_scaled = normalizer.fit_transform(X_train)\n",
    "  x_test_scaled = normalizer.transform(X_test)\n",
    "\n",
    "  regr.fit(x_train_scaled, y_train)\n",
    "  prediction = regr.predict(x_test_scaled)\n",
    "  predictions.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([21.27534617])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claudia Sheinbaum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = regr\n",
    "predictions = []\n",
    "\n",
    "features_included = feature_columns_all\n",
    "\n",
    "for i in days:\n",
    "  # Scan the file and set data\n",
    "  data = pd.read_csv(f'../claudia/1_{i}.csv', usecols=columns, encoding=\"utf-8\")\n",
    "  # Training and testing data; Remove last row which is the testing row\n",
    "  training = data.iloc[:-1]\n",
    "  testing = pd.DataFrame(data.iloc[-1])\n",
    "  testing = testing.T\n",
    "\n",
    "  # Features and target columns as NumPy arrays\n",
    "  X_train = training[features_included].values\n",
    "  X_test = testing[features_included].values\n",
    "  y_train = training[target].values.ravel()  # Flatten y_train to 1D array\n",
    "  y_test = testing[target].values.ravel()\n",
    "\n",
    "  x_train_scaled = normalizer.fit_transform(X_train)\n",
    "  x_test_scaled = normalizer.transform(X_test)\n",
    "\n",
    "  regr.fit(x_train_scaled, y_train)\n",
    "  prediction = regr.predict(x_test_scaled)\n",
    "  predictions.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, array([48.83820267]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions), average(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alvarez Maynez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = regr\n",
    "predictions = []\n",
    "\n",
    "features_included = feature_columns_all\n",
    "\n",
    "for i in days:\n",
    "  # Scan the file and set data\n",
    "  data = pd.read_csv(f'../maynez/3_{i}.csv', usecols=columns, encoding=\"utf-8\")\n",
    "  # Training and testing data; Remove last row which is the testing row\n",
    "  training = data.iloc[:-1]\n",
    "  testing = pd.DataFrame(data.iloc[-1])\n",
    "  testing = testing.T\n",
    "\n",
    "  # Features and target columns as NumPy arrays\n",
    "  X_train = training[features_included].values\n",
    "  X_test = testing[features_included].values\n",
    "  y_train = training[target].values.ravel()  # Flatten y_train to 1D array\n",
    "  y_test = testing[target].values.ravel()\n",
    "\n",
    "  x_train_scaled = normalizer.fit_transform(X_train)\n",
    "  x_test_scaled = normalizer.transform(X_test)\n",
    "\n",
    "  regr.fit(x_train_scaled, y_train)\n",
    "  prediction = regr.predict(x_test_scaled)\n",
    "  predictions.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, array([7.92547477]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions), average(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
